/*
 * Tachyon API
 *
 * Tachyon Platform REST API
 *
 * The version of the OpenAPI document: 0.49.3
 *
 * Generated by: https://openapi-generator.tech
 */

use crate::models;
use serde::{Deserialize, Serialize};

/// AgentExecuteRequest : TODO: add English documentation
#[derive(Clone, Default, Debug, PartialEq, Serialize, Deserialize)]
pub struct AgentExecuteRequest {
    /// Additional description appended to each tool's help text shown to the LLM.
    #[serde(
        rename = "additional_tool_description",
        default,
        with = "::serde_with::rust::double_option",
        skip_serializing_if = "Option::is_none"
    )]
    pub additional_tool_description: Option<Option<String>>,
    /// Optional AgentProtocol ID to use for this execution.
    #[serde(
        rename = "agent_protocol_id",
        default,
        with = "::serde_with::rust::double_option",
        skip_serializing_if = "Option::is_none"
    )]
    pub agent_protocol_id: Option<Option<String>>,
    /// AgentProtocol selection mode.
    #[serde(
        rename = "agent_protocol_mode",
        skip_serializing_if = "Option::is_none"
    )]
    pub agent_protocol_mode: Option<models::AgentProtocolMode>,
    /// Display name for the assistant in the conversation.
    #[serde(
        rename = "assistant_name",
        default,
        with = "::serde_with::rust::double_option",
        skip_serializing_if = "Option::is_none"
    )]
    pub assistant_name: Option<Option<String>>,
    /// When true, the agent auto-approves tool executions without asking for confirmation.
    #[serde(
        rename = "auto_approve",
        skip_serializing_if = "Option::is_none"
    )]
    pub auto_approve: Option<bool>,
    /// Controls chatroom name auto-generation behavior. - `first_only` (default): Generate on first message only (regardless of current name) - `always`: Always attempt to generate/update name after each message - `never`: Never auto-generate chatroom name
    #[serde(
        rename = "chatroom_name_generation",
        skip_serializing_if = "Option::is_none"
    )]
    pub chatroom_name_generation: Option<models::ChatroomNameGeneration>,
    /// User-defined tools executed on the client side.  Each tool must have a unique `name` that does not collide with server built-in tool names. When the LLM calls one of these tools, the server emits a `tool_call_pending` SSE event and blocks until the client submits the result via `POST /v1/llms/chatrooms/{chatroom_id}/agent/tool-result` (timeout: 5 minutes).  Tools with `fire_and_forget: true` do not block — the server immediately returns a success to the LLM and the client can execute the tool asynchronously.  Example: ```json {   \"client_tools\": [     {       \"name\": \"send_slack_message\",       \"description\": \"Send a message to a Slack channel\",       \"parameters\": {         \"type\": \"object\",         \"properties\": {           \"channel\": { \"type\": \"string\" },           \"text\": { \"type\": \"string\" }         },         \"required\": [\"channel\", \"text\"]       },       \"fire_and_forget\": false     }   ] } ```
    #[serde(
        rename = "client_tools",
        default,
        with = "::serde_with::rust::double_option",
        skip_serializing_if = "Option::is_none"
    )]
    pub client_tools: Option<Option<Vec<models::ClientToolDefinition>>>,
    /// Maximum number of LLM round-trips (tool calls) the agent is allowed to make before stopping.
    #[serde(
        rename = "max_requests",
        skip_serializing_if = "Option::is_none"
    )]
    pub max_requests: Option<i32>,
    /// Optional MCP Hub configuration in JSON string form.
    #[serde(
        rename = "mcp_hub_config_json",
        default,
        with = "::serde_with::rust::double_option",
        skip_serializing_if = "Option::is_none"
    )]
    pub mcp_hub_config_json: Option<Option<String>>,
    /// Model identifier. Accepts `provider/model` or just `model` (provider auto-detected).  Examples: - `anthropic/claude-3-sonnet-20241022` - `openai/gpt-4` - `google_ai/gemini-pro`  Auto-detection shortcuts: - `gpt-*` → OpenAI - `claude-*` → Anthropic - `gemini*` → Google AI
    #[serde(
        rename = "model",
        default,
        with = "::serde_with::rust::double_option",
        skip_serializing_if = "Option::is_none"
    )]
    pub model: Option<Option<String>>,
    /// The task or prompt for the agent to execute.
    #[serde(rename = "task")]
    pub task: String,
    /// Per-category tool access flags.
    #[serde(
        rename = "tool_access",
        skip_serializing_if = "Option::is_none"
    )]
    pub tool_access: Option<Box<models::AgentToolAccessRequest>>,
    /// When true, use JSON Schema tool definitions (function calling) instead of XML-based tool parsing. This is automatically enabled when `client_tools` is provided.
    #[serde(
        rename = "use_json_tool_calls",
        default,
        with = "::serde_with::rust::double_option",
        skip_serializing_if = "Option::is_none"
    )]
    pub use_json_tool_calls: Option<Option<bool>>,
    /// Custom system-level instructions appended to the agent's prompt.
    #[serde(
        rename = "user_custom_instructions",
        default,
        with = "::serde_with::rust::double_option",
        skip_serializing_if = "Option::is_none"
    )]
    pub user_custom_instructions: Option<Option<String>>,
}

impl AgentExecuteRequest {
    /// TODO: add English documentation
    pub fn new(task: String) -> AgentExecuteRequest {
        AgentExecuteRequest {
            additional_tool_description: None,
            agent_protocol_id: None,
            agent_protocol_mode: None,
            assistant_name: None,
            auto_approve: None,
            chatroom_name_generation: None,
            client_tools: None,
            max_requests: None,
            mcp_hub_config_json: None,
            model: None,
            task,
            tool_access: None,
            use_json_tool_calls: None,
            user_custom_instructions: None,
        }
    }
}
